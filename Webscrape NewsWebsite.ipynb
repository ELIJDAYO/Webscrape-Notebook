{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9afd0fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "baseURL=\"https://www.bbc.co.uk/search?q=\"\n",
    "#baseURL=\"https://www.bbc.co.uk/search?q={keyword}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aa81d99",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/25341945/check-if-string-has-date-any-format\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cdb71b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da6161a7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#make sure space is replaced by +\n",
    "#keywords = [\"japan+earthquake\"] \n",
    "keywords = [\"march+12\"] \n",
    "\n",
    "\n",
    "cond_month = [\"March\"] #condition month\n",
    "cond_lday = 11 #condition lower day\n",
    "cond_uday = 12 #condition upper day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a560ebb3",
   "metadata": {},
   "source": [
    "# : Get content of current page"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e29c73e",
   "metadata": {},
   "source": [
    "def get_result(url): \n",
    "    webLinks = []\n",
    "    html = requests.get(url) \n",
    "    soup = bs(html.content,'html.parser')\n",
    "    content = soup.findAll(\"p\")\n",
    "    if content is None:\n",
    "        return 'Not found'\n",
    "    else:\n",
    "        for output in content:\n",
    "            print(\"***********************\")\n",
    "            print(output.text)\n",
    "            webLinks.append(output.get('href'))\n",
    "        print(webLinks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d80573",
   "metadata": {},
   "source": [
    "# : 1.2 Select only news article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9a7616bb",
   "metadata": {
    "code_folding": [
     0,
     10,
     20,
     23
    ]
   },
   "outputs": [],
   "source": [
    "def get_resultSnippets(url):\n",
    "    snippets = []\n",
    "    html = requests.get(url) \n",
    "    soup_snippets = bs(html.content,'html.parser')\n",
    "    snippets = soup_snippets.findAll(\"dd\", {\"class\",\"ssrcss-m5j4pi-MetadataContent e1ojgjhb0\"})   \n",
    "    if snippets is None:#Note: assume there is always result or content, so it will never pass empty var\n",
    "        return snippets\n",
    "    else:\n",
    "        return snippets\n",
    "\n",
    "def get_resultLinks(url):    \n",
    "    weblinks = []\n",
    "    html = requests.get(url) \n",
    "    soup_links = bs(html.content,'html.parser')\n",
    "    rawTags = soup_links.findAll(\"a\", {\"class\",\"ssrcss-1ynlzyd-PromoLink e1f5wbog0\"})\n",
    "    for link in rawTags:\n",
    "        weblinks.append(link.get('href'))  \n",
    "\n",
    "    return weblinks\n",
    "\n",
    "def addYear(outputs, index):\n",
    "    outputs.insert(index, '99 January 9999')\n",
    "    \n",
    "def addRegion(outputs, index):\n",
    "    outputs.insert(index, ' ')\n",
    "    \n",
    "def isBlacklisted(string):\n",
    "    blacklists = ['Picture','Also','Programmes', \n",
    "              'blog', \n",
    "              'Magazine','Service'] \n",
    "    if(string==''):\n",
    "        return True\n",
    "    for blacklist in blacklists:\n",
    "        if(blacklist.upper() in string):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def removeDuplicates(rightLinks):\n",
    "    list(set(rightLinks))\n",
    "\n",
    "def filterRightLinks(rightLinks): #so that we can access the same type of website\n",
    "    blacklists=['html','blog','.stm','live']\n",
    "    i=0\n",
    "    while(i<len(rightLinks)):\n",
    "        for blacklist in blacklists:\n",
    "            if(blacklist in rightLinks[i]):\n",
    "                rightLinks.pop(i)\n",
    "                i-=1\n",
    "        i+=1\n",
    "    removeDuplicates(rightLinks)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8ba9f5c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def getRightLinks(snippets,rightLinks,weblinks):\n",
    "    i = 0\n",
    "    while(i<len(snippets)):\n",
    "        errMsg=\"None\"\n",
    "        date = snippets[i].text\n",
    "#        print(\"Show date \", date)\n",
    "        if(is_date(date)): #\n",
    "            dateArr = date.split(\" \", 3)\n",
    "            size = len(dateArr)\n",
    "            if(size == 3): #bec news with missing year will return video instead of article\n",
    "                day = int(dateArr[0])\n",
    "                month = dateArr[1]\n",
    "                if(cond_month[0] == month): #update this in future if you want month range\n",
    "\n",
    "                    if(cond_lday <= day and day <= cond_uday):\n",
    "                        i += 1\n",
    "\n",
    "                        program = snippets[i].text\n",
    "                        if(program == \"News\"): #when 21 April 2015 | Sport\n",
    "                            i+=1\n",
    "                            region = snippets[i].text\n",
    "                            if(not region.isspace()): #when 21 April | News | \\s\n",
    "                                                                \n",
    "                                if(not isBlacklisted(region.upper())):\n",
    "                                    erMsg=\"index \"+ str(i)+ \" [\"+program+ \"] [\"+ region+ \"]  Found an article\" #for debugging\n",
    "                                    indexLink = int(i/3)\n",
    "                                    rightLinks.append(weblinks[indexLink])\n",
    "                                else:\n",
    "                                    errMsg=\"index \"+ str(i)+ \" \"+program+ \" - Found blacklist\" #for debugging\n",
    "                                    pass\n",
    "                            elif(is_date(region)):\n",
    "                                addRegion(snippets,i)\n",
    "                                errMsg=\"index \"+ str(i)+ \" \"+program+\" - inserted region with empty val\"\n",
    "                            else:\n",
    "                                errMsg=\"index \"+ str(i)+ \" \"+program+\" - it was space char\"\n",
    "                                pass\n",
    "\n",
    "                        else:\n",
    "                            i+=1\n",
    "                            region = snippets[i].text\n",
    "                            errMsg=\"index \"+ str(i)+ \" \"+program+ \" - not news\" #for debugging\n",
    "                            if(not region.isspace()):\n",
    "                                pass\n",
    "                            elif(is_date(region)):\n",
    "                                addRegion(snippets,i)\n",
    "                                errMsg=\"index \"+ str(i)+ \" \"+program+ \" - inserted region with empty val\"\n",
    "                            else:\n",
    "                                errMsg=\"index \"+ str(i)+ \" \"+program+\" - it was space char\"\n",
    "                                pass\n",
    "                            \n",
    "                    else:\n",
    "                        errMsg=\"index \"+ str(i)+ \" \"+date+ \" - wrong day\" #for debugging\n",
    "                        i+=2\n",
    "                else:\n",
    "                    errMsg=\"index \"+ str(i)+ \" \"+date+ \" - wrong month\" #for debugging\n",
    "                    i+=2\n",
    "            else:\n",
    "                errMsg=\"index \"+ str(i)+ \" \"+date+ \" - missing year\" #for debugging\n",
    "                i+=2\n",
    "\n",
    "        elif(\"day\" in date): #snippets can return me # days ago so I want to remove it, in this project I'm not considering this\n",
    "            snippets.pop(i)\n",
    "            addYear(snippets,i)\n",
    "            errMsg=\"index \"+ str(i)+ \" \"+date+ \" - day ago, not date format\" #for debugging\n",
    "            i+=2\n",
    "            \n",
    "        elif(\"hour\" in date): #snippets can return me # hour ago so I want to remove it, in this project I'm not considering this\n",
    "            snippets.pop(i)\n",
    "            addYear(snippets,i)\n",
    "            errMsg=\"index \"+ str(i)+ \" \"+date+ \" - hour ago not date format\" #for debugging\n",
    "            i+=2\n",
    "        else:\n",
    "            addYear(snippets,i)\n",
    "            date = snippets[i]\n",
    "            errMsg=\"index \"+ str(i)+ \" \"+date+ \" - not date format\" #for debugging\n",
    "            i+=2\n",
    "#        print(errMsg)\n",
    "        i+=1\n",
    "\n",
    "        \n",
    "        \n",
    "#    print(\"There are \",len(snippets))\n",
    "\n",
    "\n",
    "#is date? clear\n",
    "#split year\n",
    "#is march\n",
    "#is 11-12\n",
    "#is News"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85967563",
   "metadata": {},
   "source": [
    "#this cell is just for debugging\n",
    "\n",
    "html = requests.get(\"https://www.bbc.co.uk/search?q=japan+earthquake&page=28\") \n",
    "soup_time = bs(html.content,'html.parser')\n",
    "soup_genre = bs(html.content,'html.parser')\n",
    "soup_region = bs(html.content,'html.parser')\n",
    "\n",
    "contents_genre = soup_genre.findAll(\"span\", {\"class\",\"ssrcss-8g95ls-MetadataSnippet ecn1o5v2\"})\n",
    "\n",
    "for cg in contents_genre:\n",
    " #   print(link.get('href'))\n",
    "    print(cg.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70de373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListNews(url,rightLinks):\n",
    "\n",
    "    snippets = get_resultSnippets(url)\n",
    "    weblinks = get_resultLinks(url)\n",
    "    getRightLinks(snippets,rightLinks,weblinks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26340e87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12775205",
   "metadata": {},
   "source": [
    "# : Variables to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e6c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3538d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "title = []\n",
    "fullArticle = []\n",
    "author = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec62fef",
   "metadata": {},
   "source": [
    "# : Get Last Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defdc7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ccf5188",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "page = 1\n",
    "def getLastPage(url):\n",
    "    html = requests.get(url) \n",
    "    soup = bs(html.content,'html.parser')\n",
    "    listContent = soup.findAll(\"li\",{\"class\", \"ssrcss-hp3otd-StyledListItem-PageButtonListItem e4i2y2x1\"})\n",
    "\n",
    "    if listContent is None:\n",
    "        print('Not found')\n",
    "        return 1\n",
    "    else:\n",
    "        lastPage = listContent[2].text\n",
    "#        print(lastPage)\n",
    "        return lastPage\n",
    "\n",
    "# html = requests.get(f'https://www.bbc.co.uk/search?q={keyword}&page={page}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c43f6e",
   "metadata": {},
   "source": [
    "# : Return Link with Keyword or PageNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "46562ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateLinkKeyword(baseURL, keyWord):\n",
    "    newLink = \"\"\n",
    "    newLink = baseURL+keyWord\n",
    "    return newLink\n",
    "\n",
    "def updateLinkPage(url, pageNum):\n",
    "    newLink = \"\"\n",
    "    newLink = url+\"&page=\"\n",
    "    newLink = newLink+str(pageNum)\n",
    "    return newLink\n",
    "\n",
    "#updateLink('https://www.bbc.co.uk/search?q=', keywords[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318fd01c",
   "metadata": {},
   "source": [
    "# : Combine the Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77fd979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loopPages(f_link, lastPage,rightLinks):\n",
    "    s_link = \"\" #second link s_link\n",
    "    for i in range(1, int(lastPage)+1):\n",
    "        s_link = updateLinkPage(f_link, i)\n",
    " #       print(s_link)\n",
    "        getListNews(s_link,rightLinks) #for step 1.2\n",
    "\n",
    "def loopKeywords(keywords,url,rightLinks):\n",
    "#first link f_link\n",
    "    f_link = \"\"\n",
    "    lastPage = 1\n",
    "    for keyword in keywords:\n",
    "        f_link = updateLinkKeyword(baseURL,keyword)\n",
    "        lastPage = getLastPage(f_link)\n",
    "        loopPages(f_link,lastPage,rightLinks)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be2c27",
   "metadata": {},
   "source": [
    "algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a908f8c",
   "metadata": {},
   "source": [
    "index  0   16 March\t\t\ttrue then false, i+=3\n",
    "index  1   News\t\t\t\t\n",
    "index  2   Asia\n",
    "index  3   16 March\t\t\ttrue then false, i+=3\n",
    "index  4   News\n",
    "index  5   Asia\n",
    "index  6   16 April 2016\t\ttrue\n",
    "index  7   Programmes\t\t\tfalse, i+=2\n",
    "index  8   BBC World Service\n",
    "index  9   Programmes\t\t\tfalse, insert 99 January 9999, i+=3\n",
    "index  9   99 Janury 9999\n",
    "index  10   Programmes\n",
    "index  11   BBC World Service\n",
    "index  12   18 April 2016\n",
    "index  13   News\n",
    "index  14   Asia\n",
    "index  15   18 March 2011\n",
    "index  16   News\n",
    "index  17   Asia-Pacific\n",
    "index  18   16 March 2011\t\t \n",
    "index  19   News\t\t\ttrue\n",
    "index  20   \t\t\t\tfalse\n",
    "index  21   11 March 2011\t\ttrue but no region, insert \\s at i+=2,i+=3\n",
    "Found an article\n",
    "index  22   News\n",
    "index  23   None\n",
    "index  24   12 March 2011\t\ttrue but no region, insert \\s at i+=2,i+=3\n",
    "Found an article\n",
    "index  25   News\n",
    "index  26   None\n",
    "index  27   17 July 2007\n",
    "index  28   News\n",
    "index  29   In Pictures         false bec of blcklist\n",
    "There are  30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db85bc",
   "metadata": {},
   "source": [
    "# : TestCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cbec0dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4bcf3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rightLinks = []\n",
    "loopKeywords(keywords,baseURL, rightLinks)\n",
    "filterRightLinks(rightLinks)\n",
    "#getJSON(weblinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b91bd676",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bbc.co.uk/news/uk-scotland-56320559\n",
      "https://www.bbc.co.uk/news/uk-england-43348005\n",
      "https://www.bbc.co.uk/news/21749353\n"
     ]
    }
   ],
   "source": [
    "#process links to remove blogs and videos\n",
    "\n",
    "for link in rightLinks:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec2b00",
   "metadata": {},
   "source": [
    "1. Locate Search bar and keyword will be array of strings\n",
    "2. Select or click\n",
    "3. It can go through page button list\n",
    "4. Get news article date March 11-12\n",
    "5. Save output to JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99eb280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
